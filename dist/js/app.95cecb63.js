(function(){"use strict";var t={2645:function(t,e,a){var i=a(144),n=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-app",[a("v-app-bar",{attrs:{app:"",color:"primary",dark:"",prominent:"","shrink-on-scroll":"",src:"https://picsum.photos/1920/1080?random","fade-img-on-scroll":""},scopedSlots:t._u([{key:"img",fn:function(e){var i=e.props;return[a("v-img",t._b({},"v-img",i,!1))]}},{key:"extension",fn:function(){return[a("v-tabs",{attrs:{"align-with-title":""}},[a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#bio")}}},[t._v("About Me")]),a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#news")}}},[t._v("News")]),a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#service")}}},[t._v("Services")]),a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#publication")}}},[t._v("Publications")]),a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#dataset")}}},[t._v("Datasets")]),a("v-tab",{on:{click:function(e){return t.$vuetify.goTo("#toolbox")}}},[t._v("Toolbox")])],1)]},proxy:!0}])},[a("v-app-bar-title",{staticClass:"text-no-wrap"},[t._v("ä¸°ä¸€å¸† / Yifan Feng")]),a("v-spacer")],1),a("v-main",[a("v-container",{attrs:{id:"MyContent"}},[a("MyBio",{staticClass:"my-6",attrs:{id:"bio"}}),a("v-divider"),a("MyNews",{staticClass:"my-6",attrs:{id:"news"}}),a("v-divider"),t._e(),a("MyService",{staticClass:"my-6",attrs:{id:"service"}}),a("v-divider"),a("MyPublication",{staticClass:"my-6",attrs:{id:"publication"}}),a("v-divider"),a("MyDataset",{staticClass:"my-6",attrs:{id:"dataset"}}),a("v-divider"),a("MyToolbox",{staticClass:"my-6",attrs:{id:"toolbox"}})],2),a("v-responsive",{attrs:{height:"200"}})],1),a("v-footer",{attrs:{color:"primary",dark:"",padless:""}},[a("v-container",{staticClass:"pa-0",attrs:{fluid:""}},[a("v-row",{staticClass:"align-center",attrs:{"no-gutters":""}},[a("v-col",{attrs:{cols:"12"}},[a("v-card-subtitle",{staticClass:"text-center pb-1 text-subtitle-1"},[a("strong",[t._v("Yifan Feng")])]),a("v-card-text",{staticClass:"text-center"},[t._v(" Copyright Â© 2022 - "+t._s((new Date).getFullYear())+", All rights reserved. ")])],1)],1)],1)],1)],1)},r=[],o=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("v-container",[i("v-row",[i("v-col",{attrs:{md:"4",cols:"12"}},[i("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[i("v-card",{attrs:{"max-width":"400",width:"400"}},[i("v-img",{attrs:{src:a(2988),"aspect-ratio":8/7}})],1)],1),i("v-col",{staticClass:"text-center",attrs:{cols:"12"}},[i("p",{staticClass:"text-h4"},[t._v("ä¸°ä¸€å¸† (Yifan Feng)")]),i("p",{staticClass:"text-h5 grey--text text--darken-1"},[t._v(" Ph.D. of School of Software ")]),i("p",{staticClass:"text-h6"},[i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.tsinghua.edu.cn/",target:"_blank"}},[t._v("Tsinghua University")])])]),i("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[i("v-btn",{staticClass:"mx-4",attrs:{href:"https://scholar.google.com.hk/citations?user=WntYF-sAAAAJ&hl=zh-CN",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-school ")])],1),i("v-btn",{staticClass:"mx-4",attrs:{href:"https://github.com/yifanfeng97/",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-github ")])],1),i("v-btn",{staticClass:"mx-4",attrs:{href:"mailto:evanfeng97@qq.com",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-email ")])],1)],1)],1),i("v-col",{attrs:{md:"8",cols:"12"}},[i("v-row",[i("v-col",{staticClass:"text-h4",attrs:{cols:"12"}},[t._v(" Biography ")]),i("v-col",{staticClass:"text-h6 font-weight-regular",attrs:{cols:"12"}},[i("p",[t._v(" I am currently a first-year Ph.D. student in the "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.thss.tsinghua.edu.cn/",target:"_blank"}},[t._v("School of Software")]),t._v(" at "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.tsinghua.edu.cn/",target:"_blank"}},[t._v("Tsinghua University")]),t._v(". My Ph.D supervisor is Associate Professor "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/",target:"_blank"}},[t._v("Yue Gao")]),t._v(". I have received the B.E. degree in computer science and technology from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"http://www.xidian.edu.cn/",target:"_blank"}},[t._v("Xidian University")]),t._v(" in 2018, and the M.S. degree in intelligent science and technology supervised by Professor "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://mac.xmu.edu.cn/",target:"_blank"}},[t._v("Rongrong Ji")]),t._v(" from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.xmu.edu.cn/",target:"_blank"}},[t._v("Xiamen University")]),t._v(" in 2021. My research interests include 3D Object Recognition, 3D Object Retrieval, Open-set 3D Object Retrieval, Hypergraph Neural Networks, and Complex Network. I have released the "),i("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph (DHG)")]),t._v(" toolkit for Learning with Graph Neural Networks and Hypergraph Neural Networks. If you are interested in my research, please email me at "),i("a",{staticClass:"text-decoration-none",attrs:{href:"mailto:evanfeng97@gmail.com"}},[t._v("evanfeng97@gmail.com")]),t._v(" or "),i("a",{staticClass:"text-decoration-none",attrs:{href:"mailto:evanfeng97@qq.com"}},[t._v("evanfeng97@qq.com")]),t._v(". ")])])],1),i("v-row",[i("v-col",{attrs:{md:"12",cols:"12"}},[i("p",{staticClass:"text-h5"},[t._v("Experiments")]),i("v-timeline",{staticClass:"text-body1 font-weight-regular",attrs:{dense:""}},[i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2014 - 2018")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v(" B.E. in Computer Science, Xidian University. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2017 - 2018")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v(" Intern in Institute of Deep Learning, Baidu Inc. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2018 - 2021")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v("M.S. in Intelligent Science and Technology from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://mac.xmu.edu.cn/",target:"_blank"}},[t._v("MAC")]),t._v(", Xiamen University. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2021 - Now")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v("Ph.D. of School of Software from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/",target:"_blank"}},[t._v("iMoon Lab")]),t._v(", Tsinghua University. ")])],1)],1)],1)],1)],1)],1)],1)],1)},s=[],l={name:"MyBio",data:()=>({})},c=l,p=a(1001),d=a(3453),u=a.n(d),h=a(680),v=a(26),g=a(2102),f=a(9846),m=a(6428),b=a(2829),_=a(2877),w=a(2865),x=a(6996),y=(0,p.Z)(c,o,s,!1,null,null,null),C=y.exports;u()(y,{VBtn:h.Z,VCard:v.Z,VCol:g.Z,VContainer:f.Z,VIcon:m.Z,VImg:b.Z,VRow:_.Z,VTimeline:w.Z,VTimelineItem:x.Z});var k=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("News")])]),a("v-row",[a("v-col",{attrs:{cols:"12"}},[a("v-timeline",{staticClass:"text-h6 font-weight-regular",attrs:{dense:""}},[a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jul. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have given a talk on "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.micc.unifi.it/3dor2022/",target:"_blank"}},[t._v(" 3DOR 2022")]),t._v(" about Open-Set 3D Object Retrieval. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Aug. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have released the first version ("),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/releases/tag/v0.9.1",target:"_blank"}},[t._v("v0.9.1")]),t._v(") of "),a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph (DHG)")]),t._v(". ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jul. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accepted by Computers & Graphics 2022. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jun. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have developed an online multi-modal 3D object retrieval system. Welcome to "),a("a",{attrs:{href:"https://moon-lab.tech/vors",target:"_blank"}},[t._v("play")]),t._v(". ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" May. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE T-PAMI 2022. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Feb. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(' We organize a track "'),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("Open-Set 3D Object Retrieval")]),t._v("\" in SHREC'22. ")])],1)],1)],1)],1)],1)],1)},M=[],Z={name:"MyNews",data:()=>({})},S=Z,N=(0,p.Z)(S,k,M,!1,null,null,null),D=N.exports;u()(N,{VCol:g.Z,VContainer:f.Z,VRow:_.Z,VTimeline:w.Z,VTimelineItem:x.Z});var V=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",{attrs:{dense:""}},[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Services")])]),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Program Committee Member / Reviewer: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- Pacific-Rim Conference on Multimedia (PCM 2018)")]),a("p",[t._v(" - International Joint Conference on Artificial Intelligence (IJCAI 2020) ")]),a("p",[t._v("- IEEE International Conference on Image Processing (ICIP 2020)")]),a("p",[t._v(" - International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020) ")]),a("p",[t._v(" - CAAI International Conference on Artificial Intelligence 2022 (CICAI 2022) ")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Reviewer for Journals: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- IEEE Transactions on Pattern Analysis and Machine Intelligence")]),a("p",[t._v("- IEEE Transactions on Knowledge Discovery in Data")]),a("p",[t._v("- Journal of Visual Communication and Image Representation")]),a("p",[t._v("- IEEE Signal Processing Letters")]),a("p",[t._v("- Neurocomputing")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Talk: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v(" - Tutorial on Hypergraph Learning: Methods, Tools and Applications in Medical Image Analysis (MICCAI 2019) ")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Challenge Organizer: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- SHREC'22 Track: Open-Set 3D Object Retrieval")])])],1)],1)},I=[],R={name:"MyService",data:()=>({})},O=R,A=(0,p.Z)(O,V,I,!1,null,null,null),Y=A.exports;u()(A,{VCol:g.Z,VContainer:f.Z,VRow:_.Z});var T=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Notes")])])],1)},H=[],E={name:"MyNote",data:()=>({})},j=E,P=(0,p.Z)(j,T,H,!1,null,null,null),G=P.exports;u()(P,{VContainer:f.Z,VRow:_.Z});var F=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Publications")])]),a("v-row",t._l(t.papers,(function(e){return a("v-col",{key:e.index,staticClass:"my-2",attrs:{cols:"12"}},[a("v-row",[a("v-col",{attrs:{md:"8",cols:"12"}},[a("v-row",{staticClass:"d-flex align-center"},[a("span",{staticClass:"text-h6 font-weight-medium"},[t._v(" "+t._s(e.title)+". ")]),a("span",{staticClass:"text-h6 font-italic font-weight-regular pl-1"},[t._v(" ðŸ‘‰ðŸ¼ "+t._s(e.publication))])]),a("v-row",[a("span",{staticClass:"text-body-1 font-italic grey--text text--darken-1"},[t._v(" "+t._s(e.authors)+" ")])]),a("v-row",{staticClass:"pt-2"},[""!=e.url_pdf?a("v-btn",{staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary",href:e.url_pdf,target:"_blank"}},[t._v(" PDF ")]):t._e(),""!=e.url_code?a("v-btn",{staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary",href:e.url_code,target:"_blank"}},[t._v(" Code ")]):t._e(),a("v-dialog",{attrs:{transition:"dialog-bottom-transition","max-width":"800"},scopedSlots:t._u([{key:"activator",fn:function(i){var n=i.on,r=i.attrs;return[""!=e.bib_tex?a("v-btn",t._g(t._b({staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary"},on:{click:function(a){return t.select_paper(e.bib_tex)}}},"v-btn",r,!1),n),[t._v(" Cite ")]):t._e()]}},{key:"default",fn:function(e){return[a("v-card",[a("v-toolbar",{staticClass:"text-h5 d-flex justify-center",attrs:{color:"primary",dark:""}},[a("span",[t._v("Cite This Paper!")])]),a("v-container",{staticClass:"pa-12"},[a("v-row",[a("v-row",{staticClass:"mb-6",attrs:{dense:"","no-gutters":""}},[a("v-col",{attrs:{cols:"12"}},[a("span",{staticClass:"text-h6"},[t._v(" APA: ")])]),a("v-col",{attrs:{cols:"12"}},[a("span",{domProps:{innerHTML:t._s(t.cur_bibliography)}})])],1),a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" BibTex: ")]),a("v-col",{attrs:{cols:"12"}},[a("span",{domProps:{innerHTML:t._s(t.cur_bibtex)}})])],1)],1)],1),a("v-card-actions",{staticClass:"justify-end"},[a("v-btn",{attrs:{text:""},on:{click:function(t){e.value=!1}}},[t._v("Close")])],1)],1)]}}],null,!0)})],1)],1),a("v-col",{staticClass:"pa-md-0",attrs:{xl:"2",lg:"3",md:"4",cols:"12"}},[a("v-card",[a("v-img",{attrs:{src:e.fw_path,height:"130",contain:""}})],1)],1)],1)],1)})),1)],1)},J=[];const L=a(4644);var B={name:"MyPublication",created:function(){this.papers.sort((function(t,e){return e.index-t.index}))},data:()=>({papers:[{index:1,title:"GVCNN: Group-view Convolutional Neural Networks for 3D Shape Recognition",publication:"CVPR 2018",authors:"Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji and Yue Gao*",url_pdf:"http://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.pdf",url_code:"",bib_tex:"publication/2018-gvcnn/cite.bib",fw_path:"publication/2018-gvcnn/fw.png"},{index:2,title:"PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition",publication:"MM 2018",authors:"Haoxuan You, Yifan Feng, Rongrong Ji and Yue Gao*",url_pdf:"https://arxiv.org/pdf/1808.07659",url_code:"https://github.com/Hxyou/HLWD",bib_tex:"publication/2018-pvnet/cite.bib",fw_path:"publication/2018-pvnet/fw.png"},{index:3,title:"Hypergraph Neural Networks",publication:"AAAI 2019",authors:"Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/download/4235/4113",url_code:"https://github.com/iMoonLab/HGNN",bib_tex:"publication/2019-hgnn/cite.bib",fw_path:"publication/2019-hgnn/fw.png"},{index:4,title:"PVRNet: Point-View Relation Neural Network for 3D Shape Recognition",publication:"AAAI 2019",authors:"Haoxuan You, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/view/4945/4818",url_code:"https://github.com/iMoonLab/PVRNet",bib_tex:"publication/2019-pvrnet/cite.bib",fw_path:"publication/2019-pvrnet/fw.png"},{index:5,title:"MeshNet: Mesh Neural Network for 3D Shape Representation",publication:"AAAI 2019",authors:"Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/view/4840/4713",url_code:"https://github.com/iMoonLab/MeshNet",bib_tex:"publication/2019-meshnet/cite.bib",fw_path:"publication/2019-meshnet/fw.png"},{index:6,title:"Dynamic Hypergraph Neural Networks",publication:"IJCAI 2019",authors:"Jianwen Jiang, Yuexuan wei, Yifan Feng, Jingxuan Cao and Yue Gao*",url_pdf:"https://www.ijcai.org/Proceedings/2019/0366.pdf",url_code:"https://github.com/iMoonLab/DHGNN",bib_tex:"publication/2019-dhgnn/cite.bib",fw_path:"publication/2019-dhgnn/fw.png"},{index:7,title:"Emotion Recognition by Edge-Weighted Hypergraph Neural Network",publication:"ICIP 2019",authors:"Jingzhi Shao, Junjie Zhu, Yuxuan Wei, Yifan Feng and Xibin Zhao*",url_pdf:"https://ieeexplore.ieee.org/abstract/document/8803207/",url_code:"",bib_tex:"publication/2019-emotion/cite.bib",fw_path:"publication/2019-emotion/fw.png"},{index:8,title:"Physiological Signals-based Emotion Recognition via High-order Correlation Learning",publication:"TOMM 2019",authors:"Junjie Zhu, Yuxuan Wei, Yifan Feng, Xibin Zhao and Yue Gao*",url_pdf:"https://dl.acm.org/doi/abs/10.1145/3332374",url_code:"",bib_tex:"publication/2019-physiological/cite.bib",fw_path:"publication/2019-physiological/fw.png"},{index:9,title:"Dual Channel Hypergraph Collaborative Filtering",publication:"KDD 2020",authors:"Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang and Yue Gao*",url_pdf:"https://dl.acm.org/doi/10.1145/3394486.3403253",url_code:"",bib_tex:"publication/2020-dhcf/cite.bib",fw_path:"publication/2020-dhcf/fw.png"},{index:10,title:"HGNN+: General Hypergraph Neural Networks",publication:"IEEE T-PAMI 2022",authors:"Yue Gao, Yifan Feng, Shuyi Ji and Rongrong Ji*",url_pdf:"https://ieeexplore.ieee.org/document/9795251",url_code:"",bib_tex:"publication/2022-hgnnp/cite.bib",fw_path:"publication/2022-hgnnp/fw.png"},{index:11,title:"SHREC'22 Track: Open-Set 3D Object Retrieval",publication:"Computers & Graphics 2022",authors:"Yifan Feng, Yue Gao* and Xibin Zhao, et al.",url_pdf:"https://www.sciencedirect.com/science/article/abs/pii/S0097849322001443",url_code:"https://github.com/yifanfeng97/multi-modal-generation-for-shrec22",bib_tex:"publication/2022-shrec22/cite.bib",fw_path:"publication/2022-shrec22/fw.png"},{index:12,title:"Generating Hypergraph-Based High-Order Representations of Whole-Slide Histopathological Images for Survival Prediction",publication:"Under Review",authors:"Donglin Di, Changqing Zou, Yifan Feng, Hanyan Zhou, Rongrong Ji, Qionghai Dai and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2021-hgsurvnet/fw.png"},{index:13,title:"A Bilateral-Branch Joint Learning Framework for Multiplex Bipartite Network Embedding",publication:"Under Review",authors:"Shuyi Ji, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-combi/fw.png"},{index:14,title:"Cross-Modal 3D Shape Retrieval via Heterogeneous Dynamic Graph Representation",publication:"Under Review",authors:"Yue Dai, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-hdgr/fw.png"},{index:15,title:"Hierarchical Set-to-set Representation for 3D Cross-modal Retrieval",publication:"Under Review",authors:"Yu Jiang, Cong Hua, Yifan Feng and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-hsr/fw.png"},{index:16,title:"Multi-View Time-Series Hypergraph Learning for Action Recognition",publication:"Under Review",authors:"Nan Ma, Zhixuan Wu, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-mv-tshl/fw.png"},{index:17,title:"Hypergraph Information Bottleneck Guided Hypergraph Structure Learning",publication:"Under Review",authors:"Zizhao Zhang, Yifan Feng, Shihui Ying and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-hib/fw.png"},{index:18,title:"Open-Set 3D Object Retrieval using Structure-Driven Multi-Modal Representation",publication:"Under Review",authors:"Yifan Feng, Shuyi Ji, Yu-Shen Liu and Yue Gao*.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2022-sdm2r/fw.png"}],cur_bibtex:"",cur_bibliography:""}),methods:{select_paper:function(t){let e=new L(L.util.fetchFile(t));this.cur_bibtex=e.format("bibtex",{format:"html"}),this.cur_bibliography=e.format("bibliography",{format:"html"})}}},$=B,U=a(7118),W=a(1659),z=a(6656),X=(0,p.Z)($,F,J,!1,null,null,null),q=X.exports;u()(X,{VBtn:h.Z,VCard:v.Z,VCardActions:U.h7,VCol:g.Z,VContainer:f.Z,VDialog:W.Z,VImg:b.Z,VRow:_.Z,VToolbar:z.Z});var K=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Released Datasets")])]),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v("OS-MN40 and OS-MN40-Miss")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v(" We released two datasets in SHREC'22 Track: "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("Open-Set 3D Object Retrieval")]),t._v(". The objective of this track is to evaluate the performance of the different 3D shape retrieval algorithms under the Open-Set setting, which is an unknown-category shape retrieval task (each object contains multi-modal and multi-resolution representations: mesh, point cloud, voxel, and multi-view). In this setting, the retrieval and representation models are trained using known-category 3D objects and unknown-category 3D data are used for retrieval. This track includes two datasets, OS-MN40 and OS-MN40-Miss. Each object in OS-MN40 has complete four types of modalities and each object in OS-MN40-Miss contains incomplete data. Note that the two datasets are generated based on "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://modelnet.cs.princeton.edu/",target:"_blank"}},[t._v("ModelNet40")]),t._v(". More details can be found "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("here")]),t._v(". ")]),a("p",[a("strong",[t._v("OS-MN40:")]),t._v(" An Open-Set 3D object retrieval dataset. Each 3D object in this dataset is represented with multi-modality and multi-resolution (1024 points and 2048 points for point cloud, 32 times 32 times 32 resolution and 64 times 64 times 64 resolution for voxel, 24 views captured with 15 horizon interval cameras for multi-view and raw number face and simplified 500 faces for Mesh). Most objects in OS-MN40 are selected from the ModelNet40 dataset. OS-MN40 consists of 12309 objects from 40 classes. For each object, we provide four modalities (Point cloud, Voxel, Multi-view, and Mesh). Note that the dataset is collected for an open-set 3D retrieval setting. Thus, the categories in training and retrieval are not shared. Eight classes, including airplane, flower pot, glass box, keyboard, monitor, night stand, sink, and table, are selected for training. Other 32 classes, like bed, chair, plant, bathtub, and cup, are selected for retrieval, which are unknow-category in the training stage. ")]),a("p",[a("strong",[t._v("OS-MN-40-Miss: ")]),t._v(" This sub-dataset is constructed by random drop arbitrary modality with probability 0.4 for each object. OS-MN40-Miss is collected towards modality missing problem. ")]),a("p",[a("strong",[t._v("Downloads: ")]),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://data.shrec22.moon-lab.tech:18443/SHREC22/OS-MN40.tar.gz",target:"_blank"}},[t._v(" OS-MN40(~46G)")]),t._v(", "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://data.shrec22.moon-lab.tech:18443/SHREC22/OS-MN40-Miss.tar.gz",target:"_blank"}},[t._v("OS-MN40-Miss(~28G)")]),t._v(", "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/yifanfeng97/OS-MN40-Example",target:"_blank"}},[t._v("Example Code")]),t._v(". ")])]),a("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[a("v-img",{attrs:{src:"publication/2022-shrec22/fw.png","max-height":"300",contain:""}})],1)],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v("OS-ESB-core, OS-NTU-core and OS-MN40-core")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("Coming Soon...")])])],1)],1)},Q=[],tt={name:"MyDataset",data:()=>({})},et=tt,at=(0,p.Z)(et,K,Q,!1,null,null,null),it=at.exports;u()(at,{VCol:g.Z,VContainer:f.Z,VImg:b.Z,VRow:_.Z});var nt=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",[a("v-col",{attrs:{cols:"12"}},[a("p",{staticClass:"text-h4"},[t._v("Released Toolbox")])])],1),a("v-row",{staticClass:"text-body-1"},[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph")])]),a("v-col",{staticClass:"d-flex justify-start",attrs:{cols:"12"}},[a("a",{attrs:{href:"https://pypi.org/project/dhg/",target:"_blank"}},[a("img",{staticClass:"mr-2",attrs:{src:"https://img.shields.io/pypi/v/dhg?color=purple"}})]),a("a",{attrs:{href:"https://pypi.org/project/dhg/",target:"_blank"}},[a("img",{staticClass:"mr-2",attrs:{src:"https://img.shields.io/pypi/pyversions/dhg"}})]),a("a",{attrs:{href:"https://deephypergraph.readthedocs.io/en/latest/?badge=latest",target:"_blank"}},[a("img",{staticClass:"mr-2",attrs:{src:"https://readthedocs.org/projects/deephypergraph/badge/?version=latest"}})]),a("a",{attrs:{href:"https://pypistats.org/packages/dhg",target:"_blank"}},[a("img",{staticClass:"mr-2",attrs:{src:"https://img.shields.io/pypi/dm/dhg.svg"}})]),a("a",{attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/blob/main/LICENSE",target:"_blank"}},[a("img",{staticClass:"mr-2",attrs:{src:"https://img.shields.io/github/license/imoonlab/DeepHypergraph"}})])]),a("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[a("video",{attrs:{loop:"",muted:"",autoplay:"",playsinline:"",width:"100%",src:t.video_src},domProps:{muted:!0}},[a("p",[t._v("Your browser does not support the video tag.")])])]),a("v-col",[a("p",[a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("Homepage")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph",target:"_blank"}},[t._v("Github")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/",target:"_blank"}},[t._v("Documentation")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/en/latest/zh/overview.html",target:"_blank"}},[t._v("ä¸­æ–‡æ–‡æ¡£")])]),a("p",[t._v(" DHG (DeepHypergraph) is a deep learning library built upon "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph",target:"_blank"}},[t._v("PyTorch")]),t._v(" for learning with both Graph Neural Networks and Hypergraph Neural Networks. It is a general framework that supports both low-order and high-order message passing like "),a("strong",[t._v(" from vertex to vertex, from vertex in one domain to vertex in another domain, from vertex to hyperedge, from hyperedge to vertex, from vertex set to vertex set.")])]),a("p",[t._v(" It supports a wide variety of structures like low-order structures (simple graph, directed graph, bipartite graph, etc.), high-order structures (simple hypergraph, etc.). Various spectral-based operations (like Laplacian-based smoothing) and spatial-based operations (like message psssing from domain to domain) are integrated inside different structures. It provides multiple common metrics for performance evaluation on different tasks. Many state-of-the-art models are implemented and can be easily used for research. We also provide various visualization tools for both low-order structures and high-order structures. ")]),a("p",[t._v(" In addition, DHG's "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/en/latest/api/experiments.html",target:"_blank"}},[t._v("dhg.experiments")]),t._v(" module (that implements "),a("strong",[t._v("Auto-ML")]),t._v(" upon "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://optuna.org/",target:"_blank"}},[t._v("Optuna")]),t._v(") can help you automatically tune the hyper-parameters of your models in training and easily outperforms the state-of-the-art models. ")])])],1)],1)},rt=[],ot={name:"MyToolbox",computed:{video_src(){switch(this.$vuetify.breakpoint.name){case"xs":return a(2465);case"sm":return a(2465);case"md":return a(4255);case"lg":return a(4255);case"xl":return a(4255);default:return a(4255)}}},data:()=>({})},st=ot,lt=(0,p.Z)(st,nt,rt,!1,null,null,null),ct=lt.exports;u()(lt,{VCol:g.Z,VContainer:f.Z,VRow:_.Z});var pt={name:"App",components:{MyBio:C,MyNews:D,MyService:Y,MyNote:G,MyPublication:q,MyDataset:it,MyToolbox:ct},data:()=>({}),methods:{}},dt=pt,ut=a(7524),ht=a(8320),vt=a(7905),gt=a(1418),ft=a(899),mt=a(7877),bt=a(3857),_t=a(9762),wt=a(4227),xt=a(9848),yt=(0,p.Z)(dt,n,r,!1,null,null,null),Ct=yt.exports;u()(yt,{VApp:ut.Z,VAppBar:ht.Z,VAppBarTitle:vt.Z,VCardSubtitle:U.Qq,VCardText:U.ZB,VCol:g.Z,VContainer:f.Z,VDivider:gt.Z,VFooter:ft.Z,VImg:b.Z,VMain:mt.Z,VResponsive:bt.Z,VRow:_.Z,VSpacer:_t.Z,VTab:wt.Z,VTabs:xt.Z});var kt=a(5671),Mt=a(1846);i.Z.use(kt.Z);var Zt=new kt.Z({theme:{dark:!1,themes:{light:{primary:Mt.Z.purple,secondary:Mt.Z.grey.darken1,accent:Mt.Z.shades.black,error:Mt.Z.red.accent3},dark:{primary:Mt.Z.green,secondary:Mt.Z.grey.darken1,accent:Mt.Z.shades.black,error:Mt.Z.red.accent3}}}});i.Z.config.productionTip=!1,new i.Z({vuetify:Zt,render:t=>t(Ct)}).$mount("#app")},2988:function(t,e,a){t.exports=a.p+"img/fengyifan.82fbbf58.jpg"},2465:function(t,e,a){t.exports=a.p+"media/neuron_actions_4s_v1_text.cad974c5.mp4"},4255:function(t,e,a){t.exports=a.p+"media/neuron_actions_4s_v1_wide_text.60035faf.mp4"}},e={};function a(i){var n=e[i];if(void 0!==n)return n.exports;var r=e[i]={exports:{}};return t[i].call(r.exports,r,r.exports,a),r.exports}a.m=t,function(){var t=[];a.O=function(e,i,n,r){if(!i){var o=1/0;for(p=0;p<t.length;p++){i=t[p][0],n=t[p][1],r=t[p][2];for(var s=!0,l=0;l<i.length;l++)(!1&r||o>=r)&&Object.keys(a.O).every((function(t){return a.O[t](i[l])}))?i.splice(l--,1):(s=!1,r<o&&(o=r));if(s){t.splice(p--,1);var c=n();void 0!==c&&(e=c)}}return e}r=r||0;for(var p=t.length;p>0&&t[p-1][2]>r;p--)t[p]=t[p-1];t[p]=[i,n,r]}}(),function(){a.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return a.d(e,{a:e}),e}}(),function(){a.d=function(t,e){for(var i in e)a.o(e,i)&&!a.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(t){if("object"===typeof window)return window}}()}(),function(){a.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)}}(),function(){a.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})}}(),function(){a.p="/"}(),function(){var t={143:0};a.O.j=function(e){return 0===t[e]};var e=function(e,i){var n,r,o=i[0],s=i[1],l=i[2],c=0;if(o.some((function(e){return 0!==t[e]}))){for(n in s)a.o(s,n)&&(a.m[n]=s[n]);if(l)var p=l(a)}for(e&&e(i);c<o.length;c++)r=o[c],a.o(t,r)&&t[r]&&t[r][0](),t[r]=0;return a.O(p)},i=self["webpackChunkppage2022"]=self["webpackChunkppage2022"]||[];i.forEach(e.bind(null,0)),i.push=e.bind(null,i.push.bind(i))}();var i=a.O(void 0,[998],(function(){return a(2645)}));i=a.O(i)})();
//# sourceMappingURL=app.95cecb63.js.map