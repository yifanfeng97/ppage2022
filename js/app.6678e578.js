(function(){"use strict";var t={3691:function(t,e,a){var i=a(144),n=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-app",[a("MyHeader",{attrs:{area:t.cur_area}}),a("v-main",[a("v-container",{attrs:{id:"MyContent"}},[a("MyBio",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"bio"}}),a("v-divider"),a("MyNews",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"news"}}),a("v-divider"),a("MyPublication",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"publication"}}),a("v-divider"),a("MyService",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"service"}}),a("v-divider"),a("MyDataset",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"dataset"}}),a("v-divider"),a("MyToolbox",{directives:[{name:"intersect",rawName:"v-intersect",value:t.changeCurrentTab,expression:"changeCurrentTab"}],staticClass:"my-6",attrs:{id:"toolbox"}})],1),a("v-responsive",{attrs:{height:"200"}})],1),a("MyFooter")],1)},r=[],o=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",[a("v-app-bar",{attrs:{app:"",color:"primary",dark:"",prominent:"","shrink-on-scroll":"",src:"https://picsum.photos/1920/1080?random","fade-img-on-scroll":""},scopedSlots:t._u([{key:"img",fn:function(e){var i=e.props;return[a("v-img",t._b({},"v-img",i,!1))]}},t.$vuetify.breakpoint.mobile?null:{key:"extension",fn:function(){return[a("v-tabs",{attrs:{"align-with-title":""},model:{value:t.cur_area,callback:function(e){t.cur_area=e},expression:"cur_area"}},t._l(t.items,(function(e){return a("v-tab",{key:e.id,on:{click:function(a){return t.$vuetify.goTo(e.tag)}}},[t._v(" "+t._s(e.title)+" ")])})),1)]},proxy:!0}],null,!0)},[t.$vuetify.breakpoint.mobile?a("v-app-bar-nav-icon",{on:{click:function(e){t.drawer=!0}}}):t._e(),a("v-app-bar-title",{staticClass:"text-no-wrap"},[t._v("丰一帆 / Yifan Feng")]),a("v-spacer")],1),a("v-navigation-drawer",{attrs:{app:"",temporary:""},model:{value:t.drawer,callback:function(e){t.drawer=e},expression:"drawer"}},[a("v-list",{attrs:{nav:""}},[a("v-list-item-group",{attrs:{"active-class":"deep-purple--text text--accent-4"},model:{value:t.cur_area,callback:function(e){t.cur_area=e},expression:"cur_area"}},t._l(t.items,(function(e){return a("v-list-item",{key:e.id,staticClass:"text-subtitle-1 font-weight-bold",on:{click:function(a){return t.$vuetify.goTo(e.tag)}}},[a("v-list-item-icon",[a("v-icon",[t._v(t._s(e.icon))])],1),a("v-list-item-title",{staticClass:"text-h6"},[t._v(t._s(e.title))])],1)})),1)],1)],1)],1)},s=[],l={name:"MyHeader",props:{area:{type:Number,default:0}},computed:{cur_area:{get(){return this.area},set(t){this.$emit("update:area",t)}}},data:()=>({drawer:!1,items:[{id:0,title:"About Me",icon:"mdi-account",tag:"#bio"},{id:1,title:"News",icon:"mdi-newspaper",tag:"#news"},{id:3,title:"Publications",icon:"mdi-book",tag:"#publication"},{id:4,title:"Services",icon:"mdi-cog",tag:"#service"},{id:5,title:"Datasets",icon:"mdi-database",tag:"#dataset"},{id:6,title:"Toolbox",icon:"mdi-wrench",tag:"#toolbox"}]})},c=l,d=a(1001),p=a(3453),h=a.n(p),u=a(8320),g=a(5206),v=a(7905),m=a(6428),f=a(2829),b=a(6816),_=a(7620),w=a(7874),x=a(459),y=a(3013),C=a(5132),k=a(9762),M=a(4227),S=a(5792),N=(0,d.Z)(c,o,s,!1,null,null,null),D=N.exports;h()(N,{VAppBar:u.Z,VAppBarNavIcon:g.Z,VAppBarTitle:v.Z,VIcon:m.Z,VImg:f.Z,VList:b.Z,VListItem:_.Z,VListItemGroup:w.Z,VListItemIcon:x.Z,VListItemTitle:y.V9,VNavigationDrawer:C.Z,VSpacer:k.Z,VTab:M.Z,VTabs:S.Z});var O=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-footer",{attrs:{color:"primary",dark:"",padless:""}},[a("v-container",{staticClass:"pa-0",attrs:{fluid:""}},[a("v-row",{staticClass:"align-center",attrs:{"no-gutters":""}},[a("v-col",{attrs:{cols:"12"}},[a("v-card-subtitle",{staticClass:"text-center pb-1 text-subtitle-1"},[a("strong",[t._v("Yifan Feng")])]),a("v-card-text",{staticClass:"text-center"},[t._v(" Copyright © 2022 - "+t._s((new Date).getFullYear())+", All rights reserved. ")])],1)],1)],1)],1)},Z=[],I={name:"MyFooter"},T=I,V=a(7118),Y=a(2102),R=a(9846),E=a(899),H=a(2877),j=(0,d.Z)(T,O,Z,!1,null,null,null),G=j.exports;h()(j,{VCardSubtitle:V.Qq,VCardText:V.ZB,VCol:Y.Z,VContainer:R.Z,VFooter:E.Z,VRow:H.Z});var A=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("v-container",[i("v-row",[i("v-col",{attrs:{md:"4",cols:"12"}},[i("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[i("v-card",{attrs:{"max-width":"400",width:"400"}},[i("v-img",{attrs:{src:a(2988),"aspect-ratio":8/7}})],1)],1),i("v-col",{staticClass:"text-center",attrs:{cols:"12"}},[i("p",{staticClass:"text-h4"},[t._v("丰一帆 / Yifan Feng")]),i("p",{staticClass:"text-h5 grey--text text--darken-1"},[t._v(" Ph.D. of School of Software ")]),i("p",{staticClass:"text-h6"},[i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.tsinghua.edu.cn/",target:"_blank"}},[t._v("Tsinghua University")])])]),i("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[i("v-btn",{staticClass:"mx-4",attrs:{href:"https://scholar.google.com.hk/citations?user=WntYF-sAAAAJ&hl=zh-CN",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-school ")])],1),i("v-btn",{staticClass:"mx-4",attrs:{href:"https://github.com/yifanfeng97/",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-github ")])],1),i("v-btn",{staticClass:"mx-4",attrs:{href:"mailto:evanfeng97@qq.com",icon:""}},[i("v-icon",{attrs:{large:""}},[t._v(" mdi-email ")])],1)],1)],1),i("v-col",{attrs:{md:"8",cols:"12"}},[i("v-row",[i("v-col",{staticClass:"text-h4",attrs:{cols:"12"}},[t._v(" Biography ")]),i("v-col",{staticClass:"text-h6 font-weight-regular",attrs:{cols:"12"}},[i("p",[t._v(" I am currently a third-year Ph.D. student in the "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.thss.tsinghua.edu.cn/",target:"_blank"}},[t._v("School of Software")]),t._v(" at "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.tsinghua.edu.cn/",target:"_blank"}},[t._v("Tsinghua University")]),t._v(". My Ph.D supervisor is Associate Professor "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/",target:"_blank"}},[t._v("Yue Gao")]),t._v(". I have received the B.E. degree in computer science and technology from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"http://www.xidian.edu.cn/",target:"_blank"}},[t._v("Xidian University")]),t._v(" in 2018, and the M.S. degree in intelligent science and technology supervised by Professor "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://mac.xmu.edu.cn/",target:"_blank"}},[t._v("Rongrong Ji")]),t._v(" from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.xmu.edu.cn/",target:"_blank"}},[t._v("Xiamen University")]),t._v(" in 2021. My research interests include Hypergraph Neural Networks, Complex Network 3D Object Recognition, 3D Object Retrieval, and Open-set 3D Object Retrieval. I have released the "),i("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph (DHG)")]),t._v(" toolkit for Learning with Graph Neural Networks and Hypergraph Neural Networks. If you are interested in my research, please email me at "),i("a",{staticClass:"text-decoration-none",attrs:{href:"mailto:evanfeng97@gmail.com"}},[t._v("evanfeng97@gmail.com")]),t._v(" or "),i("a",{staticClass:"text-decoration-none",attrs:{href:"mailto:evanfeng97@qq.com"}},[t._v("evanfeng97@qq.com")]),t._v(". ")])])],1),i("v-row",[i("v-col",{attrs:{md:"12",cols:"12"}},[i("p",{staticClass:"text-h5"},[t._v("Experiments")]),i("v-timeline",{staticClass:"text-body1 font-weight-regular",attrs:{dense:""}},[i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2014 - 2018")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v(" B.E. in Computer Science, Xidian University. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2017 - 2018")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v(" Intern in Institute of Deep Learning, Baidu Inc. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2018 - 2021")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v("M.S. in Intelligent Science and Technology from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://mac.xmu.edu.cn/",target:"_blank"}},[t._v("MAC")]),t._v(", Xiamen University. ")])],1)],1),i("v-timeline-item",{attrs:{small:"","fill-dot":""}},[i("v-row",{attrs:{dense:"","no-gutters":""}},[i("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v("2021 - Now")]),i("v-col",{staticClass:"align-self-center text-body-1",attrs:{md:"10",cols:"12"}},[t._v("Ph.D. of School of Software from "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/",target:"_blank"}},[t._v("iMoon Lab")]),t._v(", Tsinghua University. ")])],1)],1)],1)],1)],1)],1)],1)],1)},P=[],L={name:"MyBio",data:()=>({})},F=L,J=a(680),B=a(26),U=a(2865),W=a(6996),z=(0,d.Z)(F,A,P,!1,null,null,null),$=z.exports;h()(z,{VBtn:J.Z,VCard:B.Z,VCol:Y.Z,VContainer:R.Z,VIcon:m.Z,VImg:f.Z,VRow:H.Z,VTimeline:U.Z,VTimelineItem:W.Z});var q=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("News")])]),a("v-row",[a("v-col",{attrs:{cols:"12"}},[a("v-timeline",{staticClass:"subtitle-1 font-weight-regular",attrs:{dense:""}},[a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Feb. 2024.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/releases/tag/v0.9.4",target:"_blank"}},[t._v("v0.9.4")]),t._v(" of "),a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DHG")]),t._v(" is released. We add 6 hypergraph datasets and fix some known bugs. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jan. 2024.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" Two paper has been accpted by ICLR 2024. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Dec. 2023.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE T-PAMI 2024. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Nov. 2023.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE T-PAMI 2023. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Oct. 2023.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" I was invited to give a talk on "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://mp.weixin.qq.com/s/2mSlWBu7NYo88SjFD8Wn8Q",target:"_blank"}},[t._v(" PRCV 2023")]),t._v(" about Hypergraph Computation: Methods and Applications. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Oct. 2023.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE TNNLS 2023. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Dec. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/releases/tag/v0.9.3",target:"_blank"}},[t._v("v0.9.3")]),t._v(" of "),a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DHG")]),t._v(" is released. We add a hypergraph dataset, fix some known bugs, and add some hypergraph operation. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Sep. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/releases/tag/v0.9.2",target:"_blank"}},[t._v("v0.9.2")]),t._v(" of "),a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DHG")]),t._v(" is released. We add 21 datasets, 6 SOTA methods, and structure and feature visualizations. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Sep. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE T-PAMI 2022. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Sep. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have given a talk on "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.micc.unifi.it/3dor2022/",target:"_blank"}},[t._v(" 3DOR 2022")]),t._v(" about Open-Set 3D Object Retrieval. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Aug. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have released the first version "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph/releases/tag/v0.9.1",target:"_blank"}},[t._v("v0.9.1")]),t._v(" of "),a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph (DHG)")]),t._v(". ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jul. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accepted by Computers & Graphics 2022. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Jun. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" We have developed an online multi-modal 3D object retrieval system. Welcome to "),a("a",{attrs:{href:"https://moon-lab.tech/vors",target:"_blank"}},[t._v("play")]),t._v(". ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" May. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(" One paper has been accpted by IEEE T-PAMI 2022. ")])],1)],1),a("v-timeline-item",{attrs:{small:"","fill-dot":""}},[a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"grey--text text--darken-2 font-weight-medium font-italic",attrs:{md:"2",cols:"12"}},[t._v(" Feb. 2022.")]),a("v-col",{staticClass:"align-self-center",attrs:{md:"10",cols:"12"}},[t._v(' We organize a track "'),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("Open-Set 3D Object Retrieval")]),t._v("\" in SHREC'22. ")])],1)],1)],1)],1)],1)],1)},X=[],Q={name:"MyNews",data:()=>({})},K=Q,tt=(0,d.Z)(K,q,X,!1,null,null,null),et=tt.exports;h()(tt,{VCol:Y.Z,VContainer:R.Z,VRow:H.Z,VTimeline:U.Z,VTimelineItem:W.Z});var at=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",{attrs:{dense:""}},[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Services")])]),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Program Committee Member / Reviewer: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- Computer Vision and Pattern Recognition Conference 2024 (CVPR 2024)")]),a("p",[t._v("- CAAI International Conference on Artificial Intelligence 2022 (CICAI 2022)")]),a("p",[t._v("- International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2020)")]),a("p",[t._v("- International Joint Conference on Artificial Intelligence (IJCAI 2020)")]),a("p",[t._v("- IEEE International Conference on Image Processing (ICIP 2020)")]),a("p",[t._v("- Pacific-Rim Conference on Multimedia (PCM 2018)")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Reviewer for Journals: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- IEEE Transactions on Pattern Analysis and Machine Intelligence")]),a("p",[t._v("- International Journal of Computer Vision")]),a("p",[t._v("- Medical Image Analysis")]),a("p",[t._v("- IEEE Transactions on Knowledge Discovery in Data")]),a("p",[t._v("- Journal of Visual Communication and Image Representation")]),a("p",[t._v("- IEEE Signal Processing Letters")]),a("p",[t._v("- Neurocomputing")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Talk: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v(" - Tutorial on Hypergraph Learning: Methods, Tools and Applications in Medical Image Analysis (MICCAI 2019) ")]),a("p",[t._v(" - Hypergraph Computation: Methods and Applications (PRCV 2023) ")])])],1),a("v-row",[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" Challenge Organizer: ")]),a("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[a("p",[t._v("- SHREC'22 Track: Open-Set 3D Object Retrieval")])])],1)],1)},it=[],nt={name:"MyService",data:()=>({})},rt=nt,ot=(0,d.Z)(rt,at,it,!1,null,null,null),st=ot.exports;h()(ot,{VCol:Y.Z,VContainer:R.Z,VRow:H.Z});var lt=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",{staticClass:"text-center"},[a("p",{staticClass:"text-h4"},[t._v("Publications")])]),a("v-row",t._l(t.papers,(function(e){return a("v-col",{key:e.index,staticClass:"my-2",attrs:{cols:"12"}},[a("v-row",[a("v-col",{attrs:{md:"8",cols:"12"}},[a("v-row",{staticClass:"d-flex align-center"},[a("span",{staticClass:"text-h6 font-weight-medium"},[t._v(" "+t._s(e.title)+". ")]),a("span",{staticClass:"text-h6 font-italic font-weight-regular pl-1"},[t._v(" 👉🏼 "+t._s(e.publication))])]),a("v-row",[a("span",{staticClass:"text-body-1 font-italic grey--text text--darken-1"},[t._v(" "+t._s(e.authors)+" ")])]),a("v-row",{staticClass:"pt-2"},[""!=e.url_pdf?a("v-btn",{staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary",href:e.url_pdf,target:"_blank"}},[t._v(" PDF ")]):t._e(),""!=e.url_code?a("v-btn",{staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary",href:e.url_code,target:"_blank"}},[t._v(" Code ")]):t._e(),a("v-dialog",{attrs:{transition:"dialog-bottom-transition","max-width":"800"},scopedSlots:t._u([{key:"activator",fn:function(i){var n=i.on,r=i.attrs;return[""!=e.bib_tex?a("v-btn",t._g(t._b({staticClass:"mr-1",attrs:{"x-small":"",outlined:"",color:"primary"},on:{click:function(a){return t.select_paper(e.bib_tex)}}},"v-btn",r,!1),n),[t._v(" Cite ")]):t._e()]}},{key:"default",fn:function(e){return[a("v-card",[a("v-toolbar",{staticClass:"text-h5 d-flex justify-center",attrs:{color:"primary",dark:""}},[a("span",[t._v("Cite This Paper!")])]),a("v-container",{staticClass:"pa-12"},[a("v-row",[a("v-row",{staticClass:"mb-6",attrs:{dense:"","no-gutters":""}},[a("v-col",{attrs:{cols:"12"}},[a("span",{staticClass:"text-h6"},[t._v(" APA: ")])]),a("v-col",{attrs:{cols:"12"}},[a("span",{domProps:{innerHTML:t._s(t.cur_bibliography)}})])],1),a("v-row",{attrs:{dense:"","no-gutters":""}},[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v(" BibTex: ")]),a("v-col",{attrs:{cols:"12"}},[a("span",{domProps:{innerHTML:t._s(t.cur_bibtex)}})])],1)],1)],1),a("v-card-actions",{staticClass:"justify-end"},[a("v-btn",{attrs:{text:""},on:{click:function(t){e.value=!1}}},[t._v("Close")])],1)],1)]}}],null,!0)})],1)],1),a("v-col",{staticClass:"pa-md-0",attrs:{xl:"2",lg:"3",md:"4",cols:"12"}},[a("v-card",[a("v-img",{attrs:{src:e.fw_path,height:"130",contain:""}})],1)],1)],1)],1)})),1)],1)},ct=[];const dt=a(4644);var pt={name:"MyPublication",created:function(){this.papers.sort((function(t,e){return e.index-t.index}))},data:()=>({papers:[{index:1,title:"GVCNN: Group-view Convolutional Neural Networks for 3D Shape Recognition",publication:"CVPR 2018",authors:"Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji and Yue Gao*",url_pdf:"http://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.pdf",url_code:"",bib_tex:"publication/2018-gvcnn/cite.bib",fw_path:"publication/2018-gvcnn/fw.png"},{index:2,title:"PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition",publication:"MM 2018",authors:"Haoxuan You, Yifan Feng, Rongrong Ji and Yue Gao*",url_pdf:"https://arxiv.org/pdf/1808.07659",url_code:"https://github.com/Hxyou/HLWD",bib_tex:"publication/2018-pvnet/cite.bib",fw_path:"publication/2018-pvnet/fw.png"},{index:3,title:"Hypergraph Neural Networks",publication:"AAAI 2019",authors:"Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/download/4235/4113",url_code:"https://github.com/iMoonLab/HGNN",bib_tex:"publication/2019-hgnn/cite.bib",fw_path:"publication/2019-hgnn/fw.png"},{index:4,title:"PVRNet: Point-View Relation Neural Network for 3D Shape Recognition",publication:"AAAI 2019",authors:"Haoxuan You, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/view/4945/4818",url_code:"https://github.com/iMoonLab/PVRNet",bib_tex:"publication/2019-pvrnet/cite.bib",fw_path:"publication/2019-pvrnet/fw.png"},{index:5,title:"MeshNet: Mesh Neural Network for 3D Shape Representation",publication:"AAAI 2019",authors:"Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao and Yue Gao*",url_pdf:"https://www.aaai.org/ojs/index.php/AAAI/article/view/4840/4713",url_code:"https://github.com/iMoonLab/MeshNet",bib_tex:"publication/2019-meshnet/cite.bib",fw_path:"publication/2019-meshnet/fw.png"},{index:6,title:"Dynamic Hypergraph Neural Networks",publication:"IJCAI 2019",authors:"Jianwen Jiang, Yuexuan wei, Yifan Feng, Jingxuan Cao and Yue Gao*",url_pdf:"https://www.ijcai.org/Proceedings/2019/0366.pdf",url_code:"https://github.com/iMoonLab/DHGNN",bib_tex:"publication/2019-dhgnn/cite.bib",fw_path:"publication/2019-dhgnn/fw.png"},{index:7,title:"Emotion Recognition by Edge-Weighted Hypergraph Neural Network",publication:"ICIP 2019",authors:"Jingzhi Shao, Junjie Zhu, Yuxuan Wei, Yifan Feng and Xibin Zhao*",url_pdf:"https://ieeexplore.ieee.org/abstract/document/8803207/",url_code:"",bib_tex:"publication/2019-emotion/cite.bib",fw_path:"publication/2019-emotion/fw.png"},{index:8,title:"Physiological Signals-based Emotion Recognition via High-order Correlation Learning",publication:"TOMM 2019",authors:"Junjie Zhu, Yuxuan Wei, Yifan Feng, Xibin Zhao and Yue Gao*",url_pdf:"https://dl.acm.org/doi/abs/10.1145/3332374",url_code:"",bib_tex:"publication/2019-physiological/cite.bib",fw_path:"publication/2019-physiological/fw.png"},{index:9,title:"Dual Channel Hypergraph Collaborative Filtering",publication:"KDD 2020",authors:"Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang and Yue Gao*",url_pdf:"https://dl.acm.org/doi/10.1145/3394486.3403253",url_code:"",bib_tex:"publication/2020-dhcf/cite.bib",fw_path:"publication/2020-dhcf/fw.png"},{index:10,title:"HGNN+: General Hypergraph Neural Networks",publication:"IEEE T-PAMI 2022",authors:"Yue Gao, Yifan Feng, Shuyi Ji and Rongrong Ji*",url_pdf:"https://ieeexplore.ieee.org/document/9795251",url_code:"https://deephypergraph.readthedocs.io/en/latest/generated/dhg.models.HGNNP.html",bib_tex:"publication/2022-hgnnp/cite.bib",fw_path:"publication/2022-hgnnp/fw.png"},{index:11,title:"SHREC'22 Track: Open-Set 3D Object Retrieval",publication:"Computers & Graphics 2022",authors:"Yifan Feng, Yue Gao* and Xibin Zhao, et al.",url_pdf:"https://www.sciencedirect.com/science/article/abs/pii/S0097849322001443",url_code:"https://github.com/yifanfeng97/multi-modal-generation-for-shrec22",bib_tex:"publication/2022-shrec22/cite.bib",fw_path:"publication/2022-shrec22/fw.png"},{index:12,title:"Generating Hypergraph-Based High-Order Representations of Whole-Slide Histopathological Images for Survival Prediction",publication:"IEEE T-PAMI 2022",authors:"Donglin Di, Changqing Zou, Yifan Feng, Hanyan Zhou, Rongrong Ji, Qionghai Dai and Yue Gao*",url_pdf:"https://ieeexplore.ieee.org/document/9903546",url_code:"",bib_tex:"publication/2022-hgsurvnet/cite.bib",fw_path:"publication/2022-hgsurvnet/fw.png"},{index:13,title:"Hierarchical Set-to-set Representation for 3D Cross-modal Retrieval",publication:"IEEE TNNLS 2023",authors:"Yu Jiang, Cong Hua, Yifan Feng and Yue Gao*",url_pdf:"https://ieeexplore.ieee.org/document/10316653",url_code:"",bib_tex:"publication/2023-hsr/cite.bib",fw_path:"publication/2023-hsr/fw.png"},{index:14,title:"Hypergraph-Based Multi-Modal Representation for Open-Set 3D Object Retrieval",publication:"IEEE T-PAMI 2023",authors:"Yifan Feng, Shuyi Ji, Yu-Shen Liu and Yue Gao*.",url_pdf:"https://ieeexplore.ieee.org/document/10319392",url_code:"https://github.com/iMoonLab/HGM2R",bib_tex:"publication/2023-sdm2r/cite.bib",fw_path:"publication/2023-sdm2r/fw.png"},{index:15,title:"Hypergraph Isomorphism Computation",publication:"IEEE T-PAMI 2024",authors:"Yifan Feng, Jiashu Han, Shihui Ying and Yue Gao*",url_pdf:"https://ieeexplore.ieee.org/abstract/document/10398457",url_code:"https://github.com/iMoonLab/HIC",bib_tex:"publication/2024-hic/cite.bib",fw_path:"publication/2024-hic/fw.jpg"},{index:16,title:"Hypergraph Dynamic System",publication:"ICLR 2024",authors:"Jielong Yan, Yifan Feng, Shihui Ying and Yue Gao*",url_pdf:"https://openreview.net/forum?id=NLbRvr840Q",url_code:"https://github.com/iMoonLab/HDS-ODE",bib_tex:"publication/2024-hode/cite.bib",fw_path:"publication/2024-hode/fw.jpg"},{index:17,title:"LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference",publication:"ICLR 2024",authors:"Yifan Feng, Yihe Luo, Shihui Ying and Yue Gao*",url_pdf:"https://openreview.net/forum?id=lHasEfGsXL",url_code:"https://github.com/iMoonLab/LightHGNN",bib_tex:"publication/2024-lighthgnn/cite.bib",fw_path:"publication/2024-lighthgnn/fw.jpg"},{index:18,title:"A Bilateral-Branch Joint Learning Framework for Multiplex Bipartite Network Embedding",publication:"Under Review",authors:"Shuyi Ji, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-combi/fw.png"},{index:19,title:"Cross-Modal 3D Shape Retrieval via Heterogeneous Dynamic Graph Representation",publication:"Under Review",authors:"Yue Dai, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-hdgr/fw.png"},{index:20,title:"Multi-View Time-Series Hypergraph Learning for Action Recognition",publication:"Under Review",authors:"Nan Ma, Zhixuan Wu, Yifan Feng and Yue Gao*, et al.",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-mv-tshl/fw.png"},{index:21,title:"Hypergraph Information Bottleneck Guided Hypergraph Structure Learning",publication:"Under Review",authors:"Zizhao Zhang, Yifan Feng, Shihui Ying and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-hib/fw.png"},{index:22,title:"Multi-Modal Temporal Hypergraph Neural Network for Flotation Condition Recognition",publication:"Under Review",authors:"Zunguan Fan, Yifan Feng, Yue Gao and Kang Wang*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-fcr/fw.jpg"},{index:23,title:"Hypergraph Collaborative Filtering with Attribute Inference",publication:"Under Review",authors:"Yutong Jiang, Chenggang Yan, Donglin Di, Shuyi Ji, Yifan Feng, Qiang Zhao, Min He and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-hcf/fw.jpg"},{index:24,title:"Graph Classification via Hypergraph",publication:"Under Review",authors:"Qingmei Tang, Yifan Feng, Zunjie Zhu,, Shihui Ying, Chenggang Yan, Wenyu Wang and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-gch/fw.jpg"},{index:25,title:"Double Degree-Guided Discrete Diffusion for Hypergraph Generation",publication:"Under Review",authors:"Tao Jin, Yifan Feng, Yuhan Gao, He Min, Lei Zhang, Chenggang Yan and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-ddhg/fw.jpg"},{index:26,title:"Bayesian Hypergraph Modeling",publication:"Under Review",authors:"Shuyi Ji, Yifan Feng, Shihui Ying and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-bhm/fw.jpg"},{index:27,title:"Kernelized Hypergraph Neural Networks",publication:"Under Review",authors:"Yifan Feng, Yifan Zhang, Shihui Ying, Shaoyi Du and Yue Gao*",url_pdf:"",url_code:"",bib_tex:"",fw_path:"publication/2024-khgnn/fw.jpg"}],cur_bibtex:"",cur_bibliography:""}),methods:{select_paper:function(t){let e=new dt(dt.util.fetchFile(t));this.cur_bibtex=e.format("bibtex",{format:"html"}),this.cur_bibliography=e.format("bibliography",{format:"html"})}}},ht=pt,ut=a(4246),gt=a(6656),vt=(0,d.Z)(ht,lt,ct,!1,null,null,null),mt=vt.exports;h()(vt,{VBtn:J.Z,VCard:B.Z,VCardActions:V.h7,VCol:Y.Z,VContainer:R.Z,VDialog:ut.Z,VImg:f.Z,VRow:H.Z,VToolbar:gt.Z});var ft=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("v-container",[i("v-row",{staticClass:"text-center"},[i("p",{staticClass:"text-h4"},[t._v("Released Datasets")])]),i("v-row",[i("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v("OS-MN40 and OS-MN40-Miss")]),i("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[i("p",[t._v(" We released two datasets in SHREC'22 Track: "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("Open-Set 3D Object Retrieval")]),t._v(". The objective of this track is to evaluate the performance of the different 3D shape retrieval algorithms under the Open-Set setting, which is an unknown-category shape retrieval task (each object contains multi-modal and multi-resolution representations: mesh, point cloud, voxel, and multi-view). In this setting, the retrieval and representation models are trained using known-category 3D objects and unknown-category 3D data are used for retrieval. This track includes two datasets, OS-MN40 and OS-MN40-Miss. Each object in OS-MN40 has complete four types of modalities and each object in OS-MN40-Miss contains incomplete data. Note that the two datasets are generated based on "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://modelnet.cs.princeton.edu/",target:"_blank"}},[t._v("ModelNet40")]),t._v(". More details can be found "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://www.moon-lab.tech/shrec22",target:"_blank"}},[t._v("here")]),t._v(". ")]),i("p",[i("strong",[t._v("OS-MN40:")]),t._v(" An Open-Set 3D object retrieval dataset. Each 3D object in this dataset is represented with multi-modality and multi-resolution (1024 points and 2048 points for point cloud, 32 times 32 times 32 resolution and 64 times 64 times 64 resolution for voxel, 24 views captured with 15 horizon interval cameras for multi-view and raw number face and simplified 500 faces for Mesh). Most objects in OS-MN40 are selected from the ModelNet40 dataset. OS-MN40 consists of 12309 objects from 40 classes. For each object, we provide four modalities (Point cloud, Voxel, Multi-view, and Mesh). Note that the dataset is collected for an open-set 3D retrieval setting. Thus, the categories in training and retrieval are not shared. Eight classes, including airplane, flower pot, glass box, keyboard, monitor, night stand, sink, and table, are selected for training. Other 32 classes, like bed, chair, plant, bathtub, and cup, are selected for retrieval, which are unknow-category in the training stage. ")]),i("p",[i("strong",[t._v("OS-MN-40-Miss: ")]),t._v(" This sub-dataset is constructed by random drop arbitrary modality with probability 0.4 for each object. OS-MN40-Miss is collected towards modality missing problem. ")]),i("p",[i("strong",[t._v("Downloads: ")]),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://data.shrec22.moon-lab.tech:18443/SHREC22/OS-MN40.tar.gz",target:"_blank"}},[t._v(" OS-MN40(~46G)")]),t._v(", "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://data.shrec22.moon-lab.tech:18443/SHREC22/OS-MN40-Miss.tar.gz",target:"_blank"}},[t._v("OS-MN40-Miss(~28G)")]),t._v(", "),i("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/yifanfeng97/OS-MN40-Example",target:"_blank"}},[t._v("Example Code")]),t._v(". ")])]),i("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[i("v-img",{attrs:{src:"publication/2022-shrec22/fw.png","max-height":"300",contain:""}})],1)],1),i("v-row",[i("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[t._v("OS-ESB-core, OS-NTU-core and OS-MN40-core")]),i("v-col",{staticClass:"text-body-1",attrs:{cols:"12"}},[i("p",[t._v(" Traditional 3D object recognition/classification task is more dependent on label information compared with the traditional 3D object retrieval task. In the 3D object recognition/classification task, a given 3D object must fall into a seen category, which significantly limits its applications in the real world. Here, we release three "),i("a",{attrs:{href:"https://moon-lab.tech/os3dor/",target:"_blank"}},[t._v("open-set 3D object retrieval")]),t._v(" datasets, OS-ESB-core, OS-NTU-core and OS-MN40-core. Each 3D object in the three datasets contains "),i("span",{staticClass:"font-weight-black"},[t._v("three modalities")]),t._v(": multi-view, point cloud, and voxel. To compress the file size, in the multi-view modality, each image has the resolution of 256x256 without a transparent background. We place a plane below the 3D object. Thus, the shadow and depth information is valid in the three datasets. The voxel and point cloud modalities are directly extracted by "),i("a",{attrs:{href:"http://www.open3d.org/",target:"_blank"}},[t._v("Open3D")]),t._v(" refering to "),i("a",{attrs:{href:"http://www.open3d.org/docs/release/python_api/open3d.geometry.VoxelGrid.html",target:"_blank"}},[t._v("here")]),t._v(" and "),i("a",{attrs:{href:"http://www.open3d.org/docs/0.7.0/python_api/open3d.geometry.sample_points_poisson_disk.html",target:"_blank"}},[t._v("here")]),t._v(', respectively. The format of the voxel data in the three datasets is ".ply", which can be analysed by the '),i("a",{attrs:{href:"http://www.open3d.org/docs/release/python_api/open3d.geometry.VoxelGrid.html",target:"_blank"}},[t._v("open3d.io.read_voxel_grid")]),t._v(" function. Those unit cooredinates of the voxel modality can be obtained by the "),i("a",{attrs:{href:"http://www.open3d.org/docs/release/python_api/open3d.geometry.VoxelGrid.html",target:"_blank"}},[t._v("get_voxels")]),t._v(" and "),i("a",{attrs:{href:"http://www.open3d.org/docs/release/python_api/open3d.geometry.VoxelGrid.html",target:"_blank"}},[t._v("grid_index")]),t._v(" functions. More details can refer to the "),i("a",{attrs:{href:"https://github.com/yifanfeng97/OS3D/blob/main/datasets.py",target:"_blank"}},[t._v("dataloader")]),t._v(" in "),i("a",{attrs:{href:"https://github.com/yifanfeng97/OS3D",target:"_blank"}},[t._v("OS3D")]),t._v(". ")]),i("p",[i("span",{staticClass:"font-weight-black"},[t._v("OS-ESB-core:")]),t._v(" Download links: "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-ESB-core/OS-ESB-core.zip",target:"_blank"}},[t._v("Multi-modal Data(~100M)")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-ESB-core/query_label.txt",target:"_blank"}},[t._v("Query Label")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-ESB-core/target_label.txt",target:"_blank"}},[t._v("Target Label")]),t._v(". This dataset contains usual engineering shapes from 41 categories like: handles, L Blocks, Oil Pans, etc. ")]),i("p",[i("span",{staticClass:"font-weight-black"},[t._v("OS-NTU-core:")]),t._v(" Download links: "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-NTU-core/OS-NTU-core.zip",target:"_blank"}},[t._v("Multi-modal Data(~260M)")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-NTU-core/query_label.txt",target:"_blank"}},[t._v("Query Label")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-NTU-core/target_label.txt",target:"_blank"}},[t._v("Target Label")]),t._v(". This dataset contains 67 usual and unusual categories like: duck, ring, tank, hammer, etc. ")]),i("p",[i("span",{staticClass:"font-weight-black"},[t._v("OS-MN40-core:")]),t._v(" Download links: "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-MN40-core/OS-MN40-core.zip",target:"_blank"}},[t._v("Multi-modal Data(~1.9G)")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-MN40-core/query_label.txt",target:"_blank"}},[t._v("Query Label")]),t._v(" | "),i("a",{attrs:{href:"https://data.shrec22.moon-lab.tech:18443/OS-3DOR/OS-MN40-core/target_label.txt",target:"_blank"}},[t._v("Target Label")]),t._v(". This dataset contains 40 usual categories like: bed, chair, etc. ")])])],1),i("v-row",{attrs:{justify:"center"}},[i("v-col",{attrs:{cols:"12",md:"6"}},[i("v-card",{staticClass:"pt-6"},[i("v-img",{staticClass:"my-3",attrs:{src:a(3257),"aspect-ratio":"2.5",contain:"","max-height":"450"}}),i("v-card-subtitle",{staticClass:"text-center font-italic text-subtitle-1 font-weight-medium"},[t._v(" Visualization of the multi-modal representations of 3D objects in OS-ESB-core dataset ")])],1)],1),i("v-col",{attrs:{cols:"12",md:"6"}},[i("v-card",{staticClass:"pt-6"},[i("v-img",{staticClass:"my-3",attrs:{src:a(7793),"aspect-ratio":"2.5",contain:"","max-height":"450"}}),i("v-card-subtitle",{staticClass:"text-center font-italic text-subtitle-1 font-weight-medium"},[t._v(" Visualization of the multi-modal representations of 3D objects in OS-NTU-core dataset ")])],1)],1),i("v-col",{attrs:{cols:"12",md:"6"}},[i("v-card",{staticClass:"pt-6"},[i("v-img",{staticClass:"my-3",attrs:{src:a(7970),"aspect-ratio":"2.5",contain:"","max-height":"450"}}),i("v-card-subtitle",{staticClass:"text-center font-italic text-subtitle-1 font-weight-medium"},[t._v(" Visualization of the multi-modal representations of 3D objects in OS-MN40-core dataset ")])],1)],1)],1)],1)},bt=[],_t={name:"MyDataset",data:()=>({})},wt=_t,xt=(0,d.Z)(wt,ft,bt,!1,null,null,null),yt=xt.exports;h()(xt,{VCard:B.Z,VCardSubtitle:V.Qq,VCol:Y.Z,VContainer:R.Z,VImg:f.Z,VRow:H.Z});var Ct=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-container",[a("v-row",[a("v-col",{attrs:{cols:"12"}},[a("p",{staticClass:"text-h4"},[t._v("Released Toolbox")])])],1),a("v-row",{staticClass:"text-body-1"},[a("v-col",{staticClass:"text-h6",attrs:{cols:"12"}},[a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("DeepHypergraph")])]),a("v-col",{staticClass:"d-inline-flex",attrs:{cols:"12"}},t._l(t.tags,(function(t){return a("a",{key:t.idx,attrs:{href:t.link,target:"_blank"}},[a("v-img",{staticClass:"mr-2",attrs:{src:t.icon,"max-height":"20px",contain:""}})],1)})),0),a("v-col",{staticClass:"d-flex justify-center",attrs:{cols:"12"}},[a("video",{attrs:{loop:"",muted:"",autoplay:"",playsinline:"",width:"100%",src:t.video_src},domProps:{muted:!0}},[a("p",[t._v("Your browser does not support the video tag.")])])]),a("v-col",[a("p",[a("a",{staticClass:"text-decoration-none",attrs:{href:"http://deephypergraph.com/",target:"_blank"}},[t._v("Homepage")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph",target:"_blank"}},[t._v("Github")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/",target:"_blank"}},[t._v("Documentation")]),t._v(" | "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/en/latest/zh/overview.html",target:"_blank"}},[t._v("中文文档")])]),a("p",[t._v(" DHG (DeepHypergraph) is a deep learning library built upon "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://github.com/iMoonLab/DeepHypergraph",target:"_blank"}},[t._v("PyTorch")]),t._v(" for learning with both Graph Neural Networks and Hypergraph Neural Networks. It is a general framework that supports both low-order and high-order message passing like "),a("strong",[t._v(" from vertex to vertex, from vertex in one domain to vertex in another domain, from vertex to hyperedge, from hyperedge to vertex, from vertex set to vertex set.")])]),a("p",[t._v(" It supports a wide variety of structures like low-order structures (simple graph, directed graph, bipartite graph, etc.), high-order structures (simple hypergraph, etc.). Various spectral-based operations (like Laplacian-based smoothing) and spatial-based operations (like message psssing from domain to domain) are integrated inside different structures. It provides multiple common metrics for performance evaluation on different tasks. Many state-of-the-art models are implemented and can be easily used for research. We also provide various visualization tools for both low-order structures and high-order structures. ")]),a("p",[t._v(" In addition, DHG's "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://deephypergraph.readthedocs.io/en/latest/api/experiments.html",target:"_blank"}},[t._v("dhg.experiments")]),t._v(" module (that implements "),a("strong",[t._v("Auto-ML")]),t._v(" upon "),a("a",{staticClass:"text-decoration-none",attrs:{href:"https://optuna.org/",target:"_blank"}},[t._v("Optuna")]),t._v(") can help you automatically tune the hyper-parameters of your models in training and easily outperforms the state-of-the-art models. ")])])],1)],1)},kt=[],Mt={name:"MyToolbox",computed:{video_src(){switch(this.$vuetify.breakpoint.name){case"xs":return a(2465);case"sm":return a(2465);case"md":return a(4255);case"lg":return a(4255);case"xl":return a(4255);default:return a(4255)}}},data:()=>({tags:[{idx:2,link:"https://pypi.org/project/dhg/",icon:"https://img.shields.io/pypi/v/dhg?color=purple"},{idx:4,link:"https://deephypergraph.readthedocs.io",icon:"https://readthedocs.org/projects/deephypergraph/badge/?version=latest"},{idx:5,link:"https://pepy.tech/badge/dhg",icon:"https://pepy.tech/badge/dhg"}]})},St=Mt,Nt=(0,d.Z)(St,Ct,kt,!1,null,null,null),Dt=Nt.exports;h()(Nt,{VCol:Y.Z,VContainer:R.Z,VImg:f.Z,VRow:H.Z});var Ot={name:"App",components:{MyHeader:D,MyFooter:G,MyBio:$,MyNews:et,MyService:st,MyPublication:mt,MyDataset:yt,MyToolbox:Dt},data:()=>({cur_area:0}),methods:{changeCurrentTab(t){t[0].isIntersecting&&("bio"==t[0].target.id?this.cur_area=0:"news"==t[0].target.id?this.cur_area=1:"publication"==t[0].target.id?this.cur_area=2:"service"==t[0].target.id?this.cur_area=3:"dataset"==t[0].target.id?this.cur_area=4:"toolbox"==t[0].target.id&&(this.cur_area=5))}}},Zt=Ot,It=a(7524),Tt=a(1418),Vt=a(7877),Yt=a(3857),Rt=a(6392),Et=a.n(Rt),Ht=a(8083),jt=(0,d.Z)(Zt,n,r,!1,null,null,null),Gt=jt.exports;h()(jt,{VApp:It.Z,VContainer:R.Z,VDivider:Tt.Z,VMain:Vt.Z,VResponsive:Yt.Z}),Et()(jt,{Intersect:Ht.Z});var At=a(5671),Pt=a(1846);i.Z.use(At.Z);var Lt=new At.Z({theme:{dark:!1,themes:{light:{primary:Pt.Z.purple,secondary:Pt.Z.grey.darken1,accent:Pt.Z.shades.black,error:Pt.Z.red.accent3},dark:{primary:Pt.Z.green,secondary:Pt.Z.grey.darken1,accent:Pt.Z.shades.black,error:Pt.Z.red.accent3}}}});i.Z.config.productionTip=!1,new i.Z({vuetify:Lt,render:t=>t(Gt)}).$mount("#app")},3257:function(t,e,a){t.exports=a.p+"img/vis_esb.8b59542c.jpg"},7970:function(t,e,a){t.exports=a.p+"img/vis_mn40.9a465c8a.jpg"},7793:function(t,e,a){t.exports=a.p+"img/vis_ntu.029d271f.jpg"},2988:function(t,e,a){t.exports=a.p+"img/fengyifan.2b01a215.jpg"},2465:function(t,e,a){t.exports=a.p+"media/neuron_actions_4s_v1_text.cad974c5.mp4"},4255:function(t,e,a){t.exports=a.p+"media/neuron_actions_4s_v1_wide_text.60035faf.mp4"}},e={};function a(i){var n=e[i];if(void 0!==n)return n.exports;var r=e[i]={exports:{}};return t[i].call(r.exports,r,r.exports,a),r.exports}a.m=t,function(){var t=[];a.O=function(e,i,n,r){if(!i){var o=1/0;for(d=0;d<t.length;d++){i=t[d][0],n=t[d][1],r=t[d][2];for(var s=!0,l=0;l<i.length;l++)(!1&r||o>=r)&&Object.keys(a.O).every((function(t){return a.O[t](i[l])}))?i.splice(l--,1):(s=!1,r<o&&(o=r));if(s){t.splice(d--,1);var c=n();void 0!==c&&(e=c)}}return e}r=r||0;for(var d=t.length;d>0&&t[d-1][2]>r;d--)t[d]=t[d-1];t[d]=[i,n,r]}}(),function(){a.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return a.d(e,{a:e}),e}}(),function(){a.d=function(t,e){for(var i in e)a.o(e,i)&&!a.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(t){if("object"===typeof window)return window}}()}(),function(){a.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)}}(),function(){a.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})}}(),function(){a.p="/"}(),function(){var t={143:0};a.O.j=function(e){return 0===t[e]};var e=function(e,i){var n,r,o=i[0],s=i[1],l=i[2],c=0;if(o.some((function(e){return 0!==t[e]}))){for(n in s)a.o(s,n)&&(a.m[n]=s[n]);if(l)var d=l(a)}for(e&&e(i);c<o.length;c++)r=o[c],a.o(t,r)&&t[r]&&t[r][0](),t[r]=0;return a.O(d)},i=self["webpackChunkppage2022"]=self["webpackChunkppage2022"]||[];i.forEach(e.bind(null,0)),i.push=e.bind(null,i.push.bind(i))}();var i=a.O(void 0,[998],(function(){return a(3691)}));i=a.O(i)})();
//# sourceMappingURL=app.6678e578.js.map